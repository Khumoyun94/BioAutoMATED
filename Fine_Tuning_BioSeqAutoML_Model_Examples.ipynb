{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to test all models with validation datasets (either held out test sets or external validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import sys\n",
    "sys.path.insert(1, './main_classes/')\n",
    "\n",
    "from CAML_wrapper import run_bioseqml\n",
    "from CAML_seqprop_helpers import *\n",
    "from CAML_integrated_design_helpers import *\n",
    "from CAML_generic_deepswarm import convert_deepswarm_input, print_summary\n",
    "from CAML_transfer_learning_helpers import transform_classification_target, transform_regression_target, fit_final_deepswarm_model\n",
    "from CAML_generic_autokeras import convert_autokeras_input\n",
    "from CAML_generic_tpot import convert_tpot_input,reformat_data_traintest\n",
    "\n",
    "import scipy.stats as sp\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autokeras\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Transfer Learning on a DeepSwarm Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/automl_py37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda2/envs/automl_py37/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1646684106.2548301 (InputLay (None, 17, 4, 1)          0         \n",
      "_________________________________________________________________\n",
      "1646684106.256098 (Conv2D)   (None, 17, 4, 64)         3200      \n",
      "_________________________________________________________________\n",
      "1646684106.276694 (Flatten)  (None, 4352)              0         \n",
      "_________________________________________________________________\n",
      "1646684106.282732 (Dense)    (None, 30)                130590    \n",
      "_________________________________________________________________\n",
      "1646684106.296522 (Dense)    (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 133,852\n",
      "Trainable params: 133,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "model is originally trainable: True\n",
      "number of layers in the model: 5\n",
      "0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x13640f5d0>, setting trainable to False\n",
      "1: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13640f710>, setting trainable to False\n",
      "2: <tensorflow.python.keras.layers.core.Flatten object at 0x13640fa90>, setting trainable to False\n",
      "3: <tensorflow.python.keras.layers.core.Dense object at 0x13640fb50>, keeping trainable = True\n",
      "4: <tensorflow.python.keras.layers.core.Dense object at 0x13640fb90>, keeping trainable = True\n"
     ]
    }
   ],
   "source": [
    "# Load DeepSwarm Model and freeze all except last two layers (randomly chose this - feel free to customize)\n",
    "final_model_path = './final_exemplars/rbs_fullset/outputs/deepswarm/binary_classification/'\n",
    "final_model_name = 'deepswarm_deploy_model.h5'\n",
    "# get sequences with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform(), 'BatchNormalizationV1': BatchNormalization()}): # , 'BatchNormalizationV1': BatchNormalization()\n",
    "    model = tf.keras.models.load_model(final_model_path + final_model_name)\n",
    "print(model.summary())\n",
    "print('model is originally trainable: ' + str(model.trainable))\n",
    "print('number of layers in the model: ' + str(len(model.layers)))\n",
    "\n",
    "# set all layers except last two dense ones to be fixed\n",
    "for layer_idx, layer in enumerate(model.layers):\n",
    "    if layer_idx > len(model.layers) - 3:\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', keeping trainable = ' + str(layer.trainable))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', setting trainable to ' + str(layer.trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set RBS data to fine-tune this model\n",
    "# Read in data file\n",
    "data_dir = './clean_data/clean/'\n",
    "file_name = 'hollerer_rbs_test.csv'\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col_name = 'seq'\n",
    "df_data_input = data_df[input_col_name]\n",
    "df_data_output = data_df['out']\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'binary_classification'\n",
    "\n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_deepswarm_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type)\n",
    "\n",
    "# transform output (target) into bins for classification\n",
    "transformed_output, transform_obj = transform_classification_target(df_data_output, multiclass = 'multiclass' in task)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into deepswarm\n",
    "# deepswarm input: numerical_data_input\n",
    "# deepswarm output: transformed_output\n",
    "X = numerical_data_input\n",
    "y = to_categorical(transformed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model now...\n",
      "Train on 24888 samples, validate on 2766 samples\n",
      "Epoch 1/30\n",
      "24888/24888 [==============================] - 2s 65us/sample - loss: 0.2009 - acc: 0.9229 - binary_crossentropy: 0.2009 - val_loss: 0.2074 - val_acc: 0.9259 - val_binary_crossentropy: 0.2074\n",
      "Epoch 2/30\n",
      "24888/24888 [==============================] - 1s 60us/sample - loss: 0.1680 - acc: 0.9368 - binary_crossentropy: 0.1680 - val_loss: 0.2124 - val_acc: 0.9226 - val_binary_crossentropy: 0.2124\n",
      "Epoch 3/30\n",
      "24888/24888 [==============================] - 1s 57us/sample - loss: 0.1465 - acc: 0.9462 - binary_crossentropy: 0.1465 - val_loss: 0.2183 - val_acc: 0.9223 - val_binary_crossentropy: 0.2183\n",
      "Epoch 4/30\n",
      "24888/24888 [==============================] - 1s 57us/sample - loss: 0.1263 - acc: 0.9564 - binary_crossentropy: 0.1263 - val_loss: 0.2235 - val_acc: 0.9197 - val_binary_crossentropy: 0.2235\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1646684106.2548301 (InputLay (None, 17, 4, 1)          0         \n",
      "_________________________________________________________________\n",
      "1646684106.256098 (Conv2D)   (None, 17, 4, 64)         3200      \n",
      "_________________________________________________________________\n",
      "1646684106.276694 (Flatten)  (None, 4352)              0         \n",
      "_________________________________________________________________\n",
      "1646684106.282732 (Dense)    (None, 30)                130590    \n",
      "_________________________________________________________________\n",
      "1646684106.296522 (Dense)    (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 133,852\n",
      "Trainable params: 130,652\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "finetune_model_name = 'fine_tune_deepswarm_deploy_model.h5'\n",
    "    \n",
    "print('Fitting final model now...')\n",
    "num_epochs = 30 # can choose how many epochs you want\n",
    "deploy_model = fit_final_deepswarm_model(model, task, num_epochs,  X, y)\n",
    "        \n",
    "# Save the final deploy trained model\n",
    "deploy_model.save(final_model_path + finetune_model_name)\n",
    "print_summary(deploy_model, final_model_path + 'fine_tune_model_topology.txt')\n",
    "print(deploy_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Transfer Learning on an AutoKeras Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Read in data file\n",
    "data_dir = './clean_data/clean/'\n",
    "file_name = 'hollerer_rbs_test.csv'\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col_name = 'seq'\n",
    "df_data_input = data_df[input_col_name]\n",
    "df_data_output = data_df['out']\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_autokeras_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type)\n",
    "transformed_output, transform_obj = transform_classification_target(df_data_output, multiclass = 'multiclass' in task)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "# autokeras input: oh_data_input\n",
    "# autokeras output: transformed_output\n",
    "X = oh_data_input\n",
    "y = transformed_output # don't convert to categorical for autokeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = './final_exemplars/rbs_fullset/models/autokeras/binary_classification/'\n",
    "final_model_name = 'optimized_autokeras_pipeline_classification.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation after no retraining:  0.9238370691732948\n",
      "Evaluation after some retraining:  0.9226319595083152\n",
      "Evaluation after training weights from scratch:  0.8920221740178357\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, np.array(y).astype(float),train_size=train_size, test_size = 1-train_size)\n",
    "\n",
    "clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after no retraining: ', evaluation)\n",
    "\n",
    "# retrain = False indicates that the weights should be reused and then retrained\n",
    "# retrain = True indicates that the weights should be reinitialized from scratch\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=False)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after some retraining: ', evaluation)\n",
    "\n",
    "# can save and reload at will\n",
    "autokeras.utils.pickle_to_file(clf, final_model_path + 'fine_tune_autokeras_pipeline_classification.h5')\n",
    "test = autokeras.utils.pickle_from_file(final_model_path+'fine_tune_autokeras_pipeline_classification.h5')\n",
    "\n",
    "# showing retrain = True wipes the old weights and ends up with a worse model\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=True)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after training weights from scratch: ', evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Transfer Learning on TPOT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unknown letter(s) \"J\" found in sequence\n",
      "Example of bad letter J: JJHKPQAKSYLAYRILDYJJ\n",
      "Replacing J with substitution : L, I\n",
      "Setting all substitutions to 1 in one-hot encoded representation...\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# read in data file\n",
    "data_dir = './clean_data/clean/'\n",
    "file_name = 'classification_test_peptides.csv'\n",
    "data_df = pd.read_csv(data_dir + file_name,sep=',')\n",
    "\n",
    "# give inputs for data generation\n",
    "input_col_name = 'seq'\n",
    "df_data_input = data_df[input_col_name]\n",
    "df_data_output = data_df['target']\n",
    "pad_seqs = False\n",
    "augment_data = 'none'\n",
    "sequence_type = 'protein'\n",
    "task = 'regression'\n",
    "\n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_tpot_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type)\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "X = numerical_data_input\n",
    "y = transformed_output # don't convert to categorical for tpot\n",
    "training_features, training_target = reformat_data_traintest(X, y)\n",
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(training_features, training_target, train_size=train_size, test_size = 1-train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give inputs for paths\n",
    "final_model_path = './final_exemplars/peptides/outputs/tpot/regression/'\n",
    "final_model_name = 'final_model_tpot_regression.pkl'\n",
    "output_folder = final_model_path\n",
    "\n",
    "with open(final_model_path+final_model_name, 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No partial_fit could be applied. Trying warm_start instead.\n",
      "\n",
      "Original model on new test data:  (0.9166148084159028, 3.488342859219614e-29)\n",
      "Keys that must be manually changed in the model to allow fine-tuning on new data: \n",
      "\tstackingestimator__estimator__n_estimators\n",
      "\tstackingestimator__estimator__warm_start\n",
      "\textratreesregressor__n_estimators\n",
      "\textratreesregressor__warm_start\n",
      "Fine-tuned model on new test data:  (0.9147476942924223, 7.255469715120219e-29)\n"
     ]
    }
   ],
   "source": [
    "# partial_fit transfer learning is only possible for models that support it - most do not\n",
    "# see reference list of those models here: https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning\n",
    "try:\n",
    "    model.partial_fit(X_train_new,y_train_new)\n",
    "except:\n",
    "    print(\"No partial_fit could be applied. Trying warm_start instead.\")\n",
    "    print(\"\")\n",
    "try:\n",
    "    # Can check out the original model parameters - should see warm_start = False\n",
    "    # print(model.get_params())\n",
    "    preds = model.predict(X_test_new)\n",
    "    \n",
    "    print('Original model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "    \n",
    "    print('Keys that must be manually changed in the model to allow fine-tuning on new data: ')\n",
    "    for key in list(model.get_params().keys()):\n",
    "        if 'warm_start' in key or 'n_estimator' in key:\n",
    "            print('\\t' + key)\n",
    "        model.set_params(stackingestimator__estimator__warm_start = True)\n",
    "        model.set_params(extratreesregressor__warm_start = True)\n",
    "        model.set_params(stackingestimator__estimator__n_estimators = 1 + model.get_params()['stackingestimator__estimator__n_estimators'])\n",
    "        model.set_params(extratreesregressor__n_estimators = 1 + model.get_params()['extratreesregressor__n_estimators'])\n",
    "    \n",
    "    # Can check out the new model parameters - should see warm_start = True and n_estimators higher \n",
    "    # n_estimators must be increased because you need to allow new estimators to be created)\n",
    "    # see reference here for more information: https://stackoverflow.com/questions/42757892/how-to-use-warm-start\n",
    "    # print(model.get_params())\n",
    "    model.fit(X_train_new,y_train_new)\n",
    "    preds = model.predict(X_test_new)    \n",
    "    print('Fine-tuned model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"No warm_start could be applied. Model is not compatible with transfer learning.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_py37",
   "language": "python",
   "name": "automl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
