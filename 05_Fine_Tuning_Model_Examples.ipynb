{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to test all models with validation datasets (either held out test sets or external validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "import sys\n",
    "sys.path.insert(1, './main_classes/')\n",
    "\n",
    "from wrapper import run_bioautomated\n",
    "from integrated_design_helpers import *\n",
    "from generic_automl_classes import convert_generic_input, read_in_data_file\n",
    "from generic_deepswarm import print_summary\n",
    "from transfer_learning_helpers import transform_classification_target, transform_regression_target, fit_final_deepswarm_model\n",
    "from generic_tpot import reformat_data_traintest\n",
    "\n",
    "import scipy.stats as sp\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autokeras\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Transfer Learning on a DeepSwarm Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1671160358.857963 (InputLaye (None, 17, 4, 1)          0         \n",
      "_________________________________________________________________\n",
      "1671160358.858701 (Conv2D)   (None, 17, 4, 32)         1600      \n",
      "_________________________________________________________________\n",
      "1671160358.871562 (Flatten)  (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "1671160358.8757818 (Dense)   (None, 256)               557312    \n",
      "_________________________________________________________________\n",
      "1671160359.024989 (Dense)    (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 559,426\n",
      "Trainable params: 559,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "model is originally trainable: True\n",
      "number of layers in the model: 5\n",
      "0: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x112249c90>, setting trainable to False\n",
      "1: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x112249f90>, setting trainable to False\n",
      "2: <tensorflow.python.keras.layers.core.Flatten object at 0x1121ee590>, setting trainable to False\n",
      "3: <tensorflow.python.keras.layers.core.Dense object at 0x143880590>, keeping trainable = True\n",
      "4: <tensorflow.python.keras.layers.core.Dense object at 0x1121fcf90>, keeping trainable = True\n"
     ]
    }
   ],
   "source": [
    "# Load DeepSwarm Model and freeze all except last two layers (randomly chose this - feel free to customize)\n",
    "final_model_path = './exemplars/rbs/outputs/deepswarm/binary_classification/'\n",
    "final_model_name = 'deepswarm_deploy_model.h5'\n",
    "# get sequences with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform(), 'BatchNormalizationV1': BatchNormalization()}): # , 'BatchNormalizationV1': BatchNormalization()\n",
    "    model = tf.keras.models.load_model(final_model_path + final_model_name)\n",
    "print(model.summary())\n",
    "print('model is originally trainable: ' + str(model.trainable))\n",
    "print('number of layers in the model: ' + str(len(model.layers)))\n",
    "\n",
    "# set all layers except last two dense ones to be fixed\n",
    "for layer_idx, layer in enumerate(model.layers):\n",
    "    if layer_idx > len(model.layers) - 3:\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', keeping trainable = ' + str(layer.trainable))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        print(str(layer_idx) + ': ' + str(layer) + ', setting trainable to ' + str(layer.trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set RBS data to fine-tune this model\n",
    "data_folder = './clean_data/clean/'\n",
    "data_file = 'hollerer_rbs_test.csv'\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'seq'\n",
    "target_col = 'out'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'binary_classification'\n",
    "model_type = 'deepswarm'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# transform output (target) into bins for classification\n",
    "transformed_output, transform_obj = transform_classification_target(df_data_output, multiclass = 'multiclass' in task)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into deepswarm\n",
    "# deepswarm input: numerical_data_input\n",
    "# deepswarm output: transformed_output\n",
    "X = numerical_data_input\n",
    "y = to_categorical(transformed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model now...\n",
      "Train on 24888 samples, validate on 2766 samples\n",
      "Epoch 1/30\n",
      "24888/24888 [==============================] - 8s 328us/sample - loss: 0.2824 - acc: 0.9122 - binary_crossentropy: 0.2824 - val_loss: 0.2153 - val_acc: 0.9168 - val_binary_crossentropy: 0.2153\n",
      "Epoch 2/30\n",
      "24888/24888 [==============================] - 7s 267us/sample - loss: 0.1396 - acc: 0.9539 - binary_crossentropy: 0.1396 - val_loss: 0.2221 - val_acc: 0.9187 - val_binary_crossentropy: 0.2221\n",
      "Epoch 3/30\n",
      "24888/24888 [==============================] - 6s 241us/sample - loss: 0.0830 - acc: 0.9767 - binary_crossentropy: 0.0830 - val_loss: 0.2398 - val_acc: 0.9176 - val_binary_crossentropy: 0.2398\n",
      "Epoch 4/30\n",
      "24888/24888 [==============================] - 6s 233us/sample - loss: 0.0476 - acc: 0.9888 - binary_crossentropy: 0.0476 - val_loss: 0.2563 - val_acc: 0.9197 - val_binary_crossentropy: 0.2563\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1671160358.857963 (InputLaye (None, 17, 4, 1)          0         \n",
      "_________________________________________________________________\n",
      "1671160358.858701 (Conv2D)   (None, 17, 4, 32)         1600      \n",
      "_________________________________________________________________\n",
      "1671160358.871562 (Flatten)  (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "1671160358.8757818 (Dense)   (None, 256)               557312    \n",
      "_________________________________________________________________\n",
      "1671160359.024989 (Dense)    (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 559,426\n",
      "Trainable params: 557,826\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "finetune_model_name = 'fine_tune_deepswarm_deploy_model.h5'\n",
    "    \n",
    "print('Fitting final model now...')\n",
    "num_epochs = 30 # can choose how many epochs you want\n",
    "deploy_model = fit_final_deepswarm_model(model, task, num_epochs,  X, y)\n",
    "        \n",
    "# Save the final deploy trained model\n",
    "deploy_model.save(final_model_path + finetune_model_name)\n",
    "print_summary(deploy_model, final_model_path + 'fine_tune_model_topology.txt')\n",
    "print(deploy_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Transfer Learning on an AutoKeras Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "data_folder = './clean_data/clean/'\n",
    "data_file = 'hollerer_rbs_test.csv'\n",
    "\n",
    "# Give inputs for data generation\n",
    "input_col = 'seq'\n",
    "target_col = 'out'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "task = 'binary_classification'\n",
    "model_type = 'autokeras'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_classification_target(df_data_output, multiclass = 'multiclass' in task)\n",
    "\n",
    "# now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "# autokeras input: oh_data_input\n",
    "# autokeras output: transformed_output\n",
    "X = oh_data_input\n",
    "y = transformed_output # don't convert to categorical for autokeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = './exemplars/rbs/models/autokeras/binary_classification/'\n",
    "final_model_name = 'optimized_autokeras_pipeline_classification.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation after no retraining:  0.91588334538443\n",
      "Evaluation after some retraining:  0.9149192576524464\n",
      "Evaluation after training weights from scratch:  0.8879248011569053\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, np.array(y).astype(float),train_size=train_size, test_size = 1-train_size)\n",
    "\n",
    "clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after no retraining: ', evaluation)\n",
    "\n",
    "# retrain = False indicates that the weights should be reused and then retrained\n",
    "# retrain = True indicates that the weights should be reinitialized from scratch\n",
    "# this may seem unintuitive but in the documentation, retrain is a boolean indicating whether or not to reinitialize the weights of the model\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=False)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after some retraining: ', evaluation)\n",
    "\n",
    "# can save and reload at will\n",
    "autokeras.utils.pickle_to_file(clf, final_model_path + 'fine_tune_autokeras_pipeline_classification.h5')\n",
    "test = autokeras.utils.pickle_from_file(final_model_path+'fine_tune_autokeras_pipeline_classification.h5')\n",
    "\n",
    "# showing retrain = True wipes the old weights and ends up with a worse model\n",
    "clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=True)\n",
    "evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "print('Evaluation after training weights from scratch: ', evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Transfer Learning on TPOT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unknown letter(s) \"X, J\" found in sequence\n",
      "Example of bad letter X: JJHKPQAKSYXPYRILDYJJ\n",
      "Example of bad letter J: JJHKPQAKSYLAYRILDYJJ\n",
      "Replacing J with substitution : L, I\n",
      "Setting all substitutions to 1 in one-hot encoded representation...\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "# read in data file\n",
    "data_folder = './clean_data/clean/'\n",
    "data_file = 'classification_test_peptides.csv'\n",
    "\n",
    "# give inputs for data generation\n",
    "input_col_name = 'seq'\n",
    "target_col = 'target'\n",
    "pad_seqs = 'max'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'protein'\n",
    "task = 'regression'\n",
    "model_type = 'tpot'\n",
    "\n",
    "# allows user to interpret model with data not in the original training set\n",
    "# so apply typical cleaning pipeline\n",
    "df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "    \n",
    "# format data inputs appropriately for autoML platform    \n",
    "numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "\n",
    "# Format data inputs appropriately for autoML platform\n",
    "transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "X = numerical_data_input\n",
    "y = transformed_output # don't convert to categorical for tpot\n",
    "training_features, training_target = reformat_data_traintest(X, y)\n",
    "train_size = 0.85\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(training_features, training_target, train_size=train_size, test_size = 1-train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give inputs for paths\n",
    "final_model_path = './exemplars/peptides/outputs/tpot/regression/'\n",
    "final_model_name = 'final_model_tpot_regression.pkl'\n",
    "output_folder = final_model_path\n",
    "\n",
    "with open(final_model_path+final_model_name, 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No partial_fit could be applied. Trying warm_start instead.\n",
      "\n",
      "Original model on new test data:  (0.6139186709056942, 1.2461480191425559e-08)\n",
      "Keys that must be manually changed in the model to allow fine-tuning on new data: \n",
      "\tn_estimators\n",
      "\twarm_start\n",
      "Fine-tuned model on new test data:  (0.6314972307097066, 3.5335892412258266e-09)\n"
     ]
    }
   ],
   "source": [
    "# partial_fit transfer learning is only possible for models that support it - most do not\n",
    "# see reference list of those models here: https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning\n",
    "try:\n",
    "    model.partial_fit(X_train_new,y_train_new)\n",
    "except:\n",
    "    print(\"No partial_fit could be applied. Trying warm_start instead.\")\n",
    "    print(\"\")\n",
    "try:\n",
    "    # Can check out the original model parameters - should see warm_start = False\n",
    "    # print(model.get_params())\n",
    "    preds = model.predict(X_test_new)\n",
    "    \n",
    "    print('Original model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "    \n",
    "    print('Keys that must be manually changed in the model to allow fine-tuning on new data: ')\n",
    "    for key in list(model.get_params().keys()):\n",
    "        if 'warm_start' in key or 'n_estimator' in key:\n",
    "            print('\\t' + key)\n",
    "        model.set_params(warm_start = True)\n",
    "        model.set_params(n_estimators = 1 + model.get_params()['n_estimators'])\n",
    "\n",
    "        #model.set_params(stackingestimator__estimator__warm_start = True)\n",
    "        #model.set_params(extratreesregressor__warm_start = True)\n",
    "        #model.set_params(stackingestimator__estimator__n_estimators = 1 + model.get_params()['stackingestimator__estimator__n_estimators'])\n",
    "        #model.set_params(extratreesregressor__n_estimators = 1 + model.get_params()['extratreesregressor__n_estimators'])\n",
    "    \n",
    "    # Can check out the new model parameters - should see warm_start = True and n_estimators higher \n",
    "    # n_estimators must be increased because you need to allow new estimators to be created)\n",
    "    # see reference here for more information: https://stackoverflow.com/questions/42757892/how-to-use-warm-start\n",
    "    # print(model.get_params())\n",
    "    model.fit(X_train_new,y_train_new)\n",
    "    preds = model.predict(X_test_new)    \n",
    "    print('Fine-tuned model on new test data: ', sp.pearsonr(y_test_new, preds))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"No warm_start could be applied. Model is not compatible with transfer learning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Transfer Learning on AutoKeras Toehold Regression Model + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression'\n",
    "pad_seqs = 'none'\n",
    "augment_data = 'none'\n",
    "sequence_type = 'nucleic_acid'\n",
    "model_type = 'autokeras'\n",
    "final_model_path = './exemplars/toeholds/models/autokeras/regression/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With retrained Green et al. models, predict on test sets (Pardee et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------  ROUND 0 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 1 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 2 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 3 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 4 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 5 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 6 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 7 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 8 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 9 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 10 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 11 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 12 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 13 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 14 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 15 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 16 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 17 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 18 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 19 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 20 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 21 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 22 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 23 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "--------------  ROUND 24 --------------\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n",
      "Confirmed: All sequence characters are in alphabet\n",
      "Confirmed: No need to pad or truncate, all sequences same length\n",
      "Confirmed: No data augmentation requested\n",
      "Confirmed: Scrambled control generated.\n"
     ]
    }
   ],
   "source": [
    "pearsons = []\n",
    "spearmans = []\n",
    "for i in range(25): # do any # of trials\n",
    "    print(\"--------------  ROUND \" + str(i) + \" --------------\")\n",
    "    # Read in Green et al. data file\n",
    "    data_folder = './clean_data/clean/'\n",
    "    data_file = 'green_sequences_toehold_test_set.csv'\n",
    "\n",
    "    # Give inputs for data generation\n",
    "    input_col = 'seq'\n",
    "    target_col = 'target'\n",
    "    final_model_name = 'optimized_autokeras_pipeline_regression.h5'\n",
    "\n",
    "    # allows user to interpret model with data not in the original training set\n",
    "    # so apply typical cleaning pipeline\n",
    "    df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)\n",
    "\n",
    "    # Format data inputs appropriately for autoML platform\n",
    "    numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "    transformed_output, transform_obj = transform_regression_target(df_data_output)\n",
    "\n",
    "    # now, we have completed the pre-processing needed to feed our data into autokeras\n",
    "    # autokeras input: oh_data_input\n",
    "    # autokeras output: transformed_output\n",
    "    X = oh_data_input\n",
    "    y = transformed_output # don't convert to categorical for autokeras\n",
    "\n",
    "    train_size = 0.9\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, np.array(y).astype(float),train_size=train_size, test_size = 1-train_size)\n",
    "\n",
    "    clf = autokeras.utils.pickle_from_file(final_model_path+final_model_name)\n",
    "    evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "\n",
    "    # retrain = False indicates that the weights should be reused and then retrained\n",
    "    # retrain = True indicates that the weights should be reinitialized from scratch\n",
    "    clf.fit(np.array(X_train_new),np.array(y_train_new), retrain=False)\n",
    "    evaluation = clf.evaluate(np.array(X_test_new), np.array(y_test_new))\n",
    "\n",
    "    # can save and reload at will\n",
    "    autokeras.utils.pickle_to_file(clf, final_model_path + 'fine_tune_autokeras_pipeline_classification.h5')\n",
    "\n",
    "    # Read in data file\n",
    "    data_dir = './clean_data/clean/'\n",
    "    file_name = 'clean_pardee_sequences_toehold_test_set.csv'\n",
    "\n",
    "    # Give inputs for data generation\n",
    "    input_col = 'seq'\n",
    "    target_col = 'rank'\n",
    "    df_data_input, df_data_output, _ = read_in_data_file(data_dir + file_name, input_col, target_col)\n",
    "        \n",
    "    # Give inputs for paths\n",
    "    final_model_name = 'fine_tune_autokeras_pipeline_classification.h5'\n",
    "\n",
    "    # Format data inputs appropriately for autoML platform \n",
    "    numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)\n",
    "    preds = AutoMLBackend.generic_predict(oh_data_input, numerical_data_input, model_type, final_model_path, final_model_name)\n",
    "\n",
    "    # We are interested in class 1\n",
    "    preddf = pd.DataFrame(preds)\n",
    "    y_pred = preddf.iloc[:,0]\n",
    "    y_true = list(df_data_output.iloc[:,0]) # use output that was scrambled in same order\n",
    "    \n",
    "    # do metrics\n",
    "    slope, intercept, r_val, p_val, std_error = sp.linregress(y_true, y_pred)\n",
    "    pearsonr = sp.pearsonr(y_true, y_pred)\n",
    "    spearman = sp.spearmanr(y_true, y_pred)\n",
    "    pearsons.append(pearsonr[0])\n",
    "    spearmans.append(spearman[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31852604570850174\n",
      "0.04956622474192118\n",
      "0.25735652173913043\n",
      "0.06663728754036465\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pearsons))\n",
    "print(np.std(pearsons))\n",
    "print(np.mean(spearmans))\n",
    "print(np.std(spearmans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_py37",
   "language": "python",
   "name": "automl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
