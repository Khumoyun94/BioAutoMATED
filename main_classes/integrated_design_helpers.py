#!/usr/bin/env python
# coding: utf-8

############## PART 1: IMPORT STATEMENTS ##############

# import system libraries
import platform
import random
import shutil
import sys
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import random
from tqdm import tqdm_notebook as tqdm
import scipy.stats as sp
from scipy.stats import sem, t
from scipy import mean
import logomaker
import matplotlib
import matplotlib.font_manager as fm
import os
from itertools import combinations_with_replacement
import sklearn.metrics
from sklearn.decomposition import PCA
import itertools
from pysster.One_Hot_Encoder import One_Hot_Encoder

# import generic functions
sys.path.insert(1, '../main_classes/')
from interpret_helpers import plot_rawseqlogos, get_one_bp_mismatches, get_new_mismatch_seqs
from generic_automl_classes import AutoMLBackend, process_glycans, checkValidity, fill, makeComplement
from constraints_for_design_helpers import *
from seqprop_helpers import *

# import tensorflow libraries
import tensorflow as tf
from tensorflow.python.saved_model import tag_constants
from tensorflow.python.framework import ops

# import keras libraries
import keras
import keras as keras
from keras.models import load_model
from keras.utils import CustomObjectScope
from keras.initializers import glorot_uniform
from keras.layers import BatchNormalization
from keras.layers import Layer, InputSpec

############## PART 2: RANDOM MUTAGENESIS FUNCTIONS ##############

def get_all_possible_kmers(k, alph, sequence_type):
    """finds all kmers for a given k and a given alphabet
    Parameters
    ----------
    k : int representing number of mismatches to allow
    alph : list representation of alphabet
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'

    Returns
    -------
    kmerlist : list of possible k-mers
    """

    kmerlist = []
    if sequence_type != 'glycan':
        for combo in combinations_with_replacement(alph, r=k):
            kmerlist.append(''.join(combo))
    else:
        for combo in combinations_with_replacement(alph, r=k):
            kmerlist.append(list(combo))
    
    return(kmerlist)

def get_kmer_mismatches(seq, alph, sequence_type, k):
    """finds all kmer mismatches for a specific sequence
    Parameters
    ----------
    seq : str (if nucleic_acid or protein; else list of strs for glycans) representing sequence
    alph : list representation of alphabet
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    k : int representing number of mismatches to allow

    Returns
    -------
    mismatches : list of mutated sequences
    """

    replacements = get_all_possible_kmers(k, alph, sequence_type)
    mismatches = []
    for e,i in enumerate(seq):
        if e < len(seq) - k:
            for b in replacements:
                if sequence_type != 'glycan':
                    new_seq = seq[:e] + b + seq[e+k:]
                else:
                    new_seq = list(seq[:e])
                    new_seq.extend([b])
                    new_seq.extend(seq[e+k:])
                mismatches.append(new_seq)
    return mismatches

def get_list_of_k_mismatches_from_list(original_list, sequence_type, alph, k, constraints, substitution_type):
    """finds all kmer mismatches and enforces constraints for a list of sequences
    Parameters
    ----------
    original_list : list of sequences
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    alph : list representation of alphabet
    k : int representing number of mismatches to allow
    constraints : list of constraint tuples generated by read_bio_constraints, with each tuple containing the following information
        constraint type, exact_seq, start_position, end_position, comp_start, comp_end
    substitution_type : str, one of 'random', 'blocked', 'constrained_random', or 'constrained_blocked'

    Returns
    -------
    original_list : list of kmer-mutated sequences with constraints enforced
    """
    
    final_list = []
    
    if 'random' in substitution_type:
        for i in range(k):
            for seq_list in [get_one_bp_mismatches(seq, alph, sequence_type) for seq in original_list]:
                for mismatch in seq_list:
                    final_list.append(mismatch)
            if 'constrained' in substitution_type:
                final_list = [enforce_bio_constraints(seq, constraints) for seq in final_list]
            if sequence_type != 'glycan':
                original_list = list(set(final_list)) # deduplicate
            else: # since lists are unhashable must deduplicate another way
                final_list = [tuple(i) for i in final_list] # deduplicate
                original_list = list(set(final_list))
                original_list = [list(i) for i in original_list]

    elif 'blocked' in substitution_type:
        for seq_list in [get_kmer_mismatches(seq, alph, sequence_type, k) for seq in original_list]:
            for mismatch in seq_list:
                final_list.append(mismatch)
        if 'constrained' in substitution_type:
            final_list = [enforce_bio_constraints(seq, constraints) for seq in final_list]
        if sequence_type != 'glycan':
            original_list = list(set(final_list)) # deduplicate
        else: # since lists are unhashable must deduplicate another way
            final_list = [tuple(i) for i in final_list] # deduplicate
            original_list = list(set(final_list))
            original_list = [list(i) for i in original_list]
    
    return original_list

def add_kmer_to_dataframe(predictions, data_df, class_of_interest, model_type = 'deepswarm'):
    """add predictions of mutated sequences to a dataframe so it can be analyzed
    Parameters
    ----------
    predictions : numpy array of predicted values
    data_df : pandas DataFrame with mismatched sequences
    class_of_interest : int corresponding to which class should be considered for predictions (for regression, must be 0)
    model_type : str, one of 'deepswarm', 'autokeras', or 'tpot' 
    
    Returns
    -------
    data_df : pandas DataFrame with mutated sequences and their predicted values
    """ 

    if model_type == 'deepswarm':
        on_preds = predictions[:,int(class_of_interest)]
        
    if model_type == 'autokeras' or model_type == 'tpot':
        on_preds = predictions
    data_df.columns = ['seqs']
    data_df['preds'] = on_preds.flatten()
    
    return(data_df)

def check_generation_method(substitution_type, seq_len, alph, k, seq_thresh = 150000):
    """check inputs for validity and decrease k if needed to reduce computational complexity
    Parameters
    ----------
    substitution_type : str, one of 'random', 'blocked', 'constrained_random', or 'constrained_blocked'
    seq_len : int representing length of the sequence
    alph : list representation of alphabet
    k : int representing number of mismatches to allow
    seq_thresh : hardcoded limit of how many mutated sequences to generate based on anticipated computatonal complexity

    Returns
    -------
    k : (potentially smaller) int representing number of mismatches to allow consider computational complexity
    """

    valid_types = ['random', 'blocked', 'constrained_random', 'constrained_blocked']
    if substitution_type not in valid_types: 
        print('substitution_type "{0}" is invalid. Using random substitution. Valid values are: {1}'.format(substitution_type, str(valid_types)))
        substitution_type = 'random'
    
    alphlen = len(alph)
    original_k = k
    if 'blocked' in substitution_type:
        guestimate_num_seqs = pow(alphlen, k) * (seq_len - 1) 
        if guestimate_num_seqs > seq_thresh:
            
            while guestimate_num_seqs > seq_thresh:
                k = k-1
                guestimate_num_seqs = pow(alphlen, k) * (seq_len - 1)
            print('k = {0} is too large for alphabet size and sequence length. Setting k = {1}'.format(original_k, k))
    elif 'random' in substitution_type:
        guestimate_num_seqs = pow(alphlen * seq_len, k)
        print(k, guestimate_num_seqs)
        if guestimate_num_seqs > seq_thresh:
            while guestimate_num_seqs > seq_thresh:
                k = k-1
                guestimate_num_seqs = pow(alphlen * seq_len, k)
            print('k = {0} is too large for alphabet size and sequence length. Setting k = {1}'.format(original_k, k))
    
    return(k)

def get_denovo_seqs_predictions(listofseqs, num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, plot_name, seq_len, model_type, k, substitution_type, class_of_interest, constraints):
    """sample list of sequences, generate all kmer-mutated sequences, run mutated seqs thru model, and generate matrix of predictions
    Parameters
    ----------
    listofseqs : numpy array of sequences converted to one-hot encoded matrix inputs 
    num_seqs_to_test : int corresponding to de_novo_num_seqs_to_test
    alph : list representation of alphabet
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    final_model_path : str representing folder with final model
    final_model_name : str representing name of final model
    plot_path : str representing folder where plots are to be located
    plot_name : str representing name of plot
    seq_len : int representing length of the sequence
    model_type : str, one of 'deepswarm', 'autokeras', or 'tpot' 
    k : int representing number of mismatches to allow
    substitution_type : str, one of 'random', 'blocked', 'constrained_random', or 'constrained_blocked'
    class_of_interest : int corresponding to which class should be considered for predictions (for regression, must be 0)
    constraints : list of constraint tuples generated by read_bio_constraints, with each tuple containing the following information
        constraint type, exact_seq, start_position, end_position, comp_start, comp_end

    Returns
    -------
    run_thru_model : pandas DataFrame with mutated sequences and their predicted values
    orig_run_model : pandas DataFrame with original sequences and their predicted values

    """ 

    if num_seqs_to_test > len(listofseqs):
        num_seqs_to_test = len(listofseqs)
    # sample lists
    listofseqs = list(listofseqs)
    sample = random.sample(listofseqs, num_seqs_to_test)
    
    sample = [np.transpose(a) for a in sample]
    # get mismatched lists
    orig = AutoMLBackend.reverse_onehot2seq(sample, alph, sequence_type, numeric = False)
    smallalph = plot_rawseqlogos(listofseqs, alph, sequence_type, plot_path, plot_name, seq_len)

    origseqs = get_list_of_k_mismatches_from_list(orig, sequence_type, smallalph, 0, constraints, substitution_type)
    mismatches = get_list_of_k_mismatches_from_list(orig, sequence_type, smallalph, k, constraints, substitution_type)

    X_num, X_oh = get_new_mismatch_seqs(mismatches, alph, sequence_type, model_type)
    X_orig_num, X_orig_oh = get_new_mismatch_seqs(origseqs, alph, sequence_type, model_type)
    
    predictions = AutoMLBackend.generic_predict(X_oh, X_num, model_type, final_model_path, final_model_name)
    orig_preds = AutoMLBackend.generic_predict(X_orig_oh, X_orig_num, model_type, final_model_path, final_model_name)

    mismatchdf = pd.DataFrame()
    mismatchdf[0] = mismatches
    run_thru_model = add_kmer_to_dataframe(predictions, mismatchdf, class_of_interest, model_type)
    
    origdf = pd.DataFrame()
    origdf[0] = origseqs
    orig_run_model = add_kmer_to_dataframe(orig_preds, origdf, class_of_interest, model_type)
    
    return(run_thru_model, orig_run_model)

############## PART 3: COSINE DISTANCE PLOTTING FOR GENERATED SEQUENCES ##############

def get_cosine(s1, s2):
    """get cosine distance between two strings
    adopted from code in https://www.codespeedy.com/similarity-metrics-of-strings-in-python/

    Parameters
    ----------
    s1 : string 1 to be compared
    s2 : string 2 to be compared

    Returns
    -------
    s3 : float of cosine distance
    """ 
    s3 = 0
    for i,j in zip(s1, s2):
        if i==j:
            s3 += 1
        else:
            s3 += 0
    s3 = s3/float(len(s1))
    
    return(s3) 

def compute_pairwise_distances(df, sequence_type, plot_path, plot_name, alph, seq_len, model_type):
    """compute pairwise distances between sets of strings and also plots raw sequence logos
    Parameters
    ----------
    df : pandas DataFrame that has been concatenated with all sequences and their predictions
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    plot_path : str representing folder where plots are to be located
    plot_name : str representing name of plot
    alph : list representation of alphabet
    seq_len : int representing length of the sequence
    model_type : str, one of 'deepswarm', 'autokeras', or 'tpot' 

    Returns
    -------
    None
    """ 

    colors = ['navy', 'cornflowerblue', 'darkorange']
    mutated_seqs = {}
    original_seqs = {}
    storm_seqs = {}
    for typeseq, typedf in df.groupby('method'):
        
        if len(typedf) > 100: # can't do pairwise cosine on too many at once
            typedf = typedf[typedf['preds'] >= np.quantile(typedf['preds'], 0.9)]

        seqs = list(typedf['seqs'])
        dists = []
        
        for i in range(len(seqs) - 1):
            j = i + 1
            dist = get_cosine(seqs[i], seqs[j])
            dists.append(dist)
        if 'Mutated' in typeseq:
            mutated_seqs[typeseq] = dists
        elif 'STORM' in typeseq:
            storm_seqs[typeseq] = dists
        else:
            original_seqs[typeseq] = dists
           
    # plot denovo seqs
    index = 0
    fig, ax = plt.subplots(figsize = (6,5), dpi = 300)
    for n, h in mutated_seqs.items():
        plt.hist(h, alpha = 1, label = n, color = colors[index % len(colors)], histtype = 'step')
        index = index + 1
    plt.legend(loc="upper left", markerscale = 1)
    ax.set_xlabel('Pairwise Cosine Distance')
    ax.set_ylabel("Number of Seqs in Top 10%")

    plt.tick_params(length = 5)
    ax.spines['left'].set_visible(True)
    ax.spines['bottom'].set_visible(True)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    plt.tight_layout()
    plt.savefig(plot_path + 'plot_denovo' + plot_name)
    plt.savefig(plot_path + 'plot_denovo' + plot_name.split('.png')[0] + '.svg')
    plt.show()
    
    # plot storm seqs
    if len(storm_seqs) > 0:
        index = 0
        fig, ax = plt.subplots(figsize = (6,5), dpi = 300)
        for n, h in storm_seqs.items():
            plt.hist(h, alpha = 1, label = n, color = colors[index % len(colors)], histtype = 'step')
            index = index + 1
        plt.legend(loc="upper left", markerscale = 1)
        ax.set_xlabel('Pairwise Cosine Distance')
        ax.set_ylabel("Number of Seqs in Top 10%")

        plt.tick_params(length = 5)
        ax.spines['left'].set_visible(True)
        ax.spines['bottom'].set_visible(True)
        ax.spines['right'].set_visible(False)
        ax.spines['top'].set_visible(False)
        plt.tight_layout()
        plt.savefig(plot_path + 'plot_storm' + plot_name)
        plt.savefig(plot_path + 'plot_storm' + plot_name.split('.png')[0] + '.svg')
        plt.show()
    
    # plot original seqs
    index = 0
    fig, ax = plt.subplots(figsize = (6,5), dpi = 300)
    for n, h in original_seqs.items():
        plt.hist(h, alpha = 1, label = n, color = colors[(index+len(mutated_seqs.keys())) % len(colors)], histtype = 'step')
        index = index + 1
    plt.legend(loc="upper left", markerscale = 1)
    ax.set_xlabel('Pairwise Cosine Distance')
    ax.set_ylabel("Number of Seqs in Top 10%")

    plt.tick_params(length = 5)
    ax.spines['left'].set_visible(True)
    ax.spines['bottom'].set_visible(True)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    plt.tight_layout()
    plt.savefig(plot_path + 'plot_original_seqs_control' + plot_name)
    plt.savefig(plot_path + 'plot_original_seqs_control' + plot_name.split('.png')[0] + '.svg')
    plt.show()
    
    for typeseq, typedf in df.groupby('method'):
        if 'Original' in typeseq:
            continue # we already plotted these
        seqs = list(typedf['seqs'])
        seqs, _ = AutoMLBackend.onehot_seqlist(seqs, sequence_type, alph, model_type)
        try:
            plot_rawseqlogos(seqs, alph, sequence_type, plot_path, 'seq_logo_' + typeseq, seq_len)
        except:
            print('No sequence logo could be computed for ' + typeseq + ' sequences.')

############## PART 4: STORM GRADIENT ASCENT / DIRECTED-DESIGN FUNCTIONS ##############

def run_gradient_ascent(input_seq, original_out, num_samples, final_model_path, seq_len, alph, target, class_of_interest, bio_constraints, transform):
    """build gradient ascent model
    adopted from code from Valeri, Collins, Ramesh et al. Nature Communications 2020 and Bogard et al. Cell 2019
    Parameters
    ----------
    input_seq : str (if nucleic_acid or protein; else list of strs for glycans) representing sequence
    original_out : float indicating original target value
    num_samples : int corresponding to how many sequences to optimize at a time
    final_model_path : str representing folder with final model
    final_model_name : str representing name of final model
    seq_len : int representing length of the sequence
    alph : list representation of alphabet
    target : float representing the target value to optimize towards
    class_of_interest : int corresponding to which class should be considered for predictions (for regression, must be 0)
    bio_constraints : list of constraint tuples generated by read_bio_constraints, with each tuple containing the following information
        constraint type, exact_seq, start_position, end_position, comp_start, comp_end
    transform : function corresponding to PWM transform as per https://nbviewer.org/github/johli/seqprop/blob/master/examples/basic/seqprop_basic_sequence_transform.ipynb

    Returns
    -------
    optimized_pwm : numpy array corresponding to optimized position weight matrix
    optimized_onehot : numpy array corresponding to the optimized PWM converted back to a one-hot matrix
    predicted_out : list of predicted target values for the optimized PWM
    """

    # build generator network
    # import features with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model
    # import features with help from https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model
    with CustomObjectScope({'GlorotUniform': glorot_uniform(), 'BatchNormalizationV1': BatchNormalization()}): # , 'BatchNormalizationV1': BatchNormalization()
        try:
            model = load_model(final_model_path)
        except:
            model = tf.keras.models.load_model(final_model_path)        
        model.load_weights(final_model_path)

    if bio_constraints is not None:
        _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=False,init_sequences = [input_seq],
                                              sequence_templates=[bio_constraints], pwm_transform_func=transform)# batch_normalize_pwm=True) # sequence_templates = bio_constraints
    else:
        _, seqprop_generator = build_generator(seq_length=seq_len, n_sequences=num_samples, batch_normalize_pwm=False,init_sequences = [input_seq],
                                              sequence_templates=None, pwm_transform_func=transform)
    # build predictor network and hook it on the generator PWM output tensor
    _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(final_model_path, seq_len, alph), final_model_path, n_sequences=num_samples, eval_mode='pwm')
    
    
    # build Loss Model (In: Generator seed, Out: Loss function)
    _, loss_model = build_loss_model(seqprop_predictor, loss_func_mod, target, class_of_interest, load_saved_predictor(final_model_path, seq_len, alph)) 
    
    # specify Optimizer to use
    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
    # compile Loss Model (Minimize self)
    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)

    # fit loss model
    callbacks =[
                EarlyStopping(monitor='loss', min_delta=0.01, patience=2, verbose=0, mode='auto'),
                #SeqPropMonitor(predictor=seqprop_predictor)#, plot_every_epoch=True, track_every_step=True, )#cse_start_pos=70, isoform_start=target_cut, isoform_end=target_cut+1, pwm_start=70-40, pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=[0])
            ]

    num_epochs=50
    train_history = loss_model.fit([], [0], epochs=num_epochs, steps_per_epoch=1000, callbacks=callbacks)

    # retrieve optimized PWMs and predicted (optimized) target
    _, optimized_pwm, optimized_onehot, predicted_out = seqprop_predictor.predict(x=None, steps=1)
    
    predicted_out = predicted_out[:,int(class_of_interest)]
    print('Original Predicted Value:', original_out)
    print('New Predicted Value: ', predicted_out)

    return optimized_pwm, optimized_onehot, predicted_out

def get_revcomp_transform_opt(rev_constraints) :
    """build transform function corresponding to PWM transform as per https://nbviewer.org/github/johli/seqprop/blob/master/examples/basic/seqprop_basic_sequence_transform.ipynb
    adopted from code from Bogard et al. Cell 2019
    Parameters
    ----------
    rev_constraints : list of constraint tuples generated by read_bio_constraints, with each tuple containing the following information
        constraint type, exact_seq, start_position, end_position, comp_start, comp_end

    Returns
    -------
    transform_func : function corresponding to PWM transform as per https://nbviewer.org/github/johli/seqprop/blob/master/examples/basic/seqprop_basic_sequence_transform.ipynb
    """
            
    def transform_func(pwm) :     
        for con in rev_constraints:
            if con[0] == 'reverse_complement':
                start1 = con[2]
                end1 = con[3]
                start2 = con[4]
                end2 = con[5]
                list_of_pos = []
                # a[1::-1]   # the first two items, reversed
                # a[:-3:-1]  # the last two items, reversed
                pwm = K.concatenate([pwm[..., :start2, :, :], K.reverse(K.concatenate([pwm[..., start1:end1+1, 1::-1, :], pwm[..., start1:end1+1, :-3:-1, :]],axis = 2), axes = 1),  pwm[..., end2+1:, :, :]], axis=-3)
        
        return(pwm)
    
    return transform_func

def get_storm_seqs_predictions(listofseqs, alph, final_model_path, final_model_name, plot_path, plot_name, seq_len, sequence_type, model_type, n_seqs_to_optimize, target_y, num_of_optimization_rounds, class_of_interest, constraints):
    """use gradient ascent model to generate new sequences and predict them
    adopted from code from Valeri, Collins, Ramesh et al. Nature Communications 2020 and Bogard et al. Cell 2019
    Parameters
    ----------
    listofseqs : numpy array of sequences converted to one-hot encoded matrix inputs 
    alph : list representation of alphabet
    final_model_path : str representing folder with final model
    final_model_name : str representing name of final model
    plot_path : str representing folder where plots are to be located
    plot_name : str representing name of plot
    seq_len : int representing length of the sequence
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    model_type : str, one of 'deepswarm', 'autokeras', or 'tpot' 
    n_seqs_to_optimize : int corresponding to storm_num_seqs_to_test
    target_y : float representing the target value to optimize towards
    num_of_optimization_rounds : int corresponding to the num_of_optimization_rounds
    class_of_interest : int corresponding to which class should be considered for predictions (for regression, must be 0)
    constraints : list of constraint tuples generated by read_bio_constraints, with each tuple containing the following information
        constraint type, exact_seq, start_position, end_position, comp_start, comp_end
    
    Returns
    -------
    best_seqs : pandas DataFrame with STORM-optimized sequences and their characteristics
    optimized_pwms : list of numpy arrays corresponding to optimized position weight matrix
    """

    if model_type != 'deepswarm':
        return(pd.DataFrame(), None)
    
    set_alph(alph, sequence_type)
    
    if n_seqs_to_optimize > len(listofseqs):
        n_seqs_to_optimize = len(listofseqs)
    
    # sample lists
    listofseqs = list(listofseqs)
    sample = random.sample(listofseqs, n_seqs_to_optimize)
    sample = [np.transpose(a) for a in sample]
    seqs = AutoMLBackend.reverse_onehot2seq(sample, alph, sequence_type, numeric = False)

    # read biological constraint template
    if constraints is not None: 
        base_seq = ['N'] * seq_len
        rev_constraints = []
        for con in constraints:
            if con[0] == 'exact_sequence':
                spec = con[1]
                start = con[2]
                end = con[3]
                base_seq[start:end+1] = spec
            elif con[0] == 'reverse_complement':
                rev_constraints.append(con)
        if sequence_type != 'glycan':
            bio_constraints = ''.join(base_seq)
        transform = get_revcomp_transform_opt(rev_constraints)
    else:
        bio_constraints = None
        transform = None

    # turn seqs back into one hot and get initial predicted values
    oh_data_input, numerical_data_input = AutoMLBackend.onehot_seqlist(seqs, sequence_type, alph, model_type = model_type)
    storm_pred_y_vals = AutoMLBackend.generic_predict(oh_data_input, numerical_data_input, model_type, final_model_path, final_model_name)
    storm_pred_y_vals = storm_pred_y_vals[:,int(class_of_interest)]
    y = np.array(storm_pred_y_vals).astype(np.float32)
    
    # setting up gradient ascent
    target = [[target_y], ] # keep in this format in case you want to adapt for multiple optimization targets

    # running gradient ascent
    optimized_pwms = [] # store the probabilities
    optimized_seqs = [] # store the converted sequences to be tested 
    predicted_targets = [] # store the original and predicted target values 

    # run n optimization rounds for each sequence- part of STORM algorithm
    num_at_a_time = 1
    for i in range(0, num_of_optimization_rounds):
        for idx, (seq, original_out) in enumerate(zip(seqs, y)): 
            optimized_pwm, _, predicted_out = run_gradient_ascent(seq, original_out, num_at_a_time, final_model_path+final_model_name, seq_len, alph, target, class_of_interest, bio_constraints, transform)
            optimized_pwms.append(np.reshape(optimized_pwm, [seq_len, len(alph)]))
            predicted_targets.append(predicted_out)
            if model_type == 'autokeras' or model_type == 'deepswarm':
                new_seq = AutoMLBackend.reverse_onehot2seq(optimized_pwm, alph, sequence_type, numeric = False)
            else:
                new_seq = AutoMLBackend.reverse_tpot2seq(optimized_pwm, alph, sequence_type)
            optimized_seqs.append(new_seq[0])
               
    # create final output table
    data_df = pd.DataFrame()
    data_df['old_seqs'] = list(itertools.chain.from_iterable(itertools.repeat(x, num_of_optimization_rounds) for x in seqs))
    data_df['old_predicted_y'] = list(itertools.chain.from_iterable(itertools.repeat(x, num_of_optimization_rounds) for x in storm_pred_y_vals))
    data_df['new_seqs'] = optimized_seqs
    data_df['STORM_predicted_y'] = predicted_targets
    data_df['optimized_pwm'] = optimized_pwms

    # use original model (not STORM model with PWM) to get fairer estimate of value 
    oh_data_input, numerical_data_input = AutoMLBackend.onehot_seqlist(optimized_seqs, sequence_type, alph, model_type = model_type)
    storm_pred_y_vals = AutoMLBackend.generic_predict(oh_data_input, numerical_data_input, model_type, final_model_path, final_model_name)
    storm_pred_y_vals = storm_pred_y_vals[:,int(class_of_interest)]
    y = np.array(storm_pred_y_vals).astype(np.float32)
    data_df['orig_model_predicted_y'] = np.reshape(y, [n_seqs_to_optimize*num_of_optimization_rounds,])

    #culling to have the best out of all optimization rounds
    # sometimes the STORM predicted value is much less than the actual predicted value due to the model re-engineering 
    # as part of STORM; the pre-trained model is hooked onto a PWM sampler at the end followed by the predictor model
    best_seqs = pd.DataFrame()
    y_col = data_df.columns.get_loc("orig_model_predicted_y") # choosing best sequence after constraints applied
    
    # cull so we have just the best out of each n rounds
    for i in range(0, n_seqs_to_optimize):
        start = i * num_of_optimization_rounds
        end = start + num_of_optimization_rounds
        best_seq_so_far = data_df.iloc[start,:]
        for j in range(start+1, end):
            curr_seq = data_df.iloc[j,:]
            lastone = abs(data_df.iloc[start, y_col] - target_y)
            currone = abs(data_df.iloc[j, y_col] - target_y)
            if (currone < lastone):
                best_seq_so_far = curr_seq
        best_seqs = pd.concat([best_seqs, best_seq_so_far], axis = 1)
    best_seqs = best_seqs.transpose()
    best_seqs.to_csv(plot_path + 'sequences_STORM_' + plot_name + '_optimized_all_info.csv', index = False)
    
    best_seqs = best_seqs[['new_seqs', 'orig_model_predicted_y']]
    best_seqs.columns = ['seqs', 'preds']
    best_seqs['preds'] = [float(x) for x in list(best_seqs['preds'])] # had to change for violinplot input
    
    return best_seqs, optimized_pwms

############## PART 5: WRAPPER FUNCTION FOR ENTIRE DESIGN MODULE ##############

def final_plot(dfplot, plot_path, plot_name):
    """wrapper function to run both random mutagenesis and gradient-ascent based design with STORM
    adopted from code from Valeri, Collins, Ramesh et al. Nature Communications 2020 and Bogard et al. Cell 2019
    Parameters
    ----------
    dfplot : pandas DataFrame with predictions (preds) and methods (method) used to generate sequences
    plot_path : str representing folder where plots are to be located
    plot_name : str representing name of plot

    Returns
    -------
    None
    """
    sns.set_style('white')
    palette = ['cornflowerblue', 'navy', 'darkorange']
    dfplot['Method Type'] = [x.split('-')[1] for x in list(dfplot['method'])]
    for name, smalldf in dfplot.groupby('Method Type'):
        fig, ax = plt.subplots(figsize = (2,2), dpi = 300)
        sns.violinplot(
            data=smalldf, x='method', y='preds',palette = palette,
            linewidth=0.25, saturation = 0.6, fliersize=1) # , inner = 'stick'
        sns.stripplot(
            data=smalldf, x='method', y='preds',palette = palette,
            linewidth=0.25, edgecolor = 'black', size = 1, alpha = 0.75) # saturation = 0.6, fliersize=1
        new_labels = [x.get_text().split('-')[0] for x in ax.get_xticklabels()]
        ax.set_xticklabels(new_labels,rotation=90)   
        ax.spines['left'].set_visible(True)
        ax.spines['bottom'].set_visible(True)
        ax.spines['right'].set_visible(False)
        ax.spines['top'].set_visible(False)
        ax.set_title(name)
        ax.set_ylabel('Predictions')
        ax.set_xlabel('Method')
        plt.tight_layout()
        plt.savefig(plot_path + name + '_violinplot' + plot_name)
        plt.savefig(plot_path + name + '_violinplot' + plot_name.split('.png')[0] + '.svg')
        plt.show()
        
    print("Computing statistics between different groups...")
    for method, smalldf in dfplot.groupby('Method Type'):
        print(method)
        original = smalldf[['Original' in name for name in smalldf['method']]]
        mutated = smalldf[['Mutated' in name for name in smalldf['method']]]
        storm = smalldf[['STORM' in name for name in smalldf['method']]]
        t = sp.ttest_ind(original['preds'], mutated['preds'])
        print('T-test comparing original to mutated: ' + str(np.round(t[0], 3)) + ' with p-val ' + str(np.round(t[1],5))) 
    
        if len(storm) > 0:
            t = sp.ttest_ind(original['preds'], storm['preds'])
            print('T-test comparing original to STORM: ' + str(np.round(t[0], 3)) + ' with p-val ' + str(np.round(t[1],5))) 
            t = sp.ttest_ind(mutated['preds'], storm['preds'])
            print('T-test comparing mutated to STORM: ' + str(np.round(t[0], 3)) + ' with p-val ' + str(np.round(t[1],5))) 

def integrated_design(numerical_data_input, oh_data_input, alph, numerical, numericalbool, final_model_path, final_model_name, plot_path, plot_name, sequence_type, model_type, design_params):
    """wrapper function to run both random mutagenesis and gradient-ascent based design with STORM
    adopted from code from Valeri, Collins, Ramesh et al. Nature Communications 2020 and Bogard et al. Cell 2019
    Parameters
    ----------
    numerical_data_input : a numpy array of sequences converted to numerical inputs 
    oh_data_input : list of sequences converted to one-hot encoded matrix inputs 
    alph : list representation of alphabet
    numerical : list of numerical output values, if the target values could be converted to floats
    numericalbool : bool representing if the target output values could be converted to floats
    final_model_path : str representing folder with final model
    final_model_name : str representing name of final model
    plot_path : str representing folder where plots are to be located
    plot_name : str representing name of plot
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    model_type : str, one of 'deepswarm', 'autokeras', or 'tpot' 
    design_params :dict of extra design parameters, with keys 'k' (int), 'substitution_type' (str), 'target_y' (float), 'class_of_interest' (int), 'constraint_file_path' (str);
        'de_novo_num_seqs_to_test' (int), 'storm_num_seqs_to_test' (int), 'num_of_optimization_rounds' (int)
    
    Returns
    -------
    None
    """

    # defaults
    k = design_params.get('k',1)
    substitution_type = design_params.get('substitution_type','random')
    target_y = design_params.get('target_y',1)
    class_of_interest = design_params.get('class_of_interest',1)
    constraint_file_path = design_params.get('constraint_file_path','')
    de_novo_num_seqs_to_test = design_params.get('de_novo_num_seqs_to_test',100)
    storm_num_seqs_to_test = design_params.get('storm_num_seqs_to_test',5)   
    num_of_optimization_rounds = design_params.get('num_of_optimization_rounds',5)

    d = list(numerical)
    howmanyclasses = len(list(set(d)))
    seq_len = len(list(oh_data_input[0][0]))
    oh = np.array(oh_data_input)

    print("Checking generation methods...")
    k = check_generation_method(substitution_type, seq_len, alph, k)
    
    print("Reading in biological constraints...")
    constraints = read_bio_constraints(constraint_file_path, alph, sequence_type)

    # get top, worst, average seqs
    classes = []
    classes_orig = []
    storm_classes = []
    classlabels = []
    avg_preds, best_preds, worst_preds = None, None, None
    if howmanyclasses < 2:
        avg_preds, orig_preds_avg = get_denovo_seqs_predictions(oh, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_average_original', seq_len, model_type, k, substitution_type, 0, constraints)
        try:
            storm_avg_preds, _ = get_storm_seqs_predictions(oh, alph, final_model_path, final_model_name, plot_path, 'average', seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, 0, constraints)
        except Exception as e:
            print(e)
            storm_avg_preds = None
            print('STORM module is not compatible with current model.')
    elif howmanyclasses < 3:
        possvals= list(set(d))
        classlabels = possvals
        for poss in possvals:
            truthvals = [d1 == poss for d1 in d]
            classx = oh[truthvals]
            classx_preds, classx_preds_orig = get_denovo_seqs_predictions(classx, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_class_' + str(poss) + '_original', seq_len, model_type, k, substitution_type, poss, constraints)
            classes.append(classx_preds)
            classes_orig.append(classx_preds_orig)
            try:
                storm_classx_preds, _ = get_storm_seqs_predictions(classx, alph, final_model_path, final_model_name, plot_path, 'class_' + str(poss), seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, poss, constraints)
                storm_classes.append(storm_classx_preds)
            except Exception as e:
                print(e)
                storm_classes = None
                print('STORM module is not compatible with current model.')
     
    else:
        if numericalbool:
            best = oh[d >= np.quantile(d, .9)]
            worst = oh[d < np.quantile(d, .1)]
            average = oh[[(x < np.quantile(d, .8) and x > np.quantile(d, 0.2)) for x in d]]
            best_preds, best_preds_orig = get_denovo_seqs_predictions(best, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_best_original', seq_len, model_type, k, substitution_type, class_of_interest, constraints)
            worst_preds, worst_preds_orig = get_denovo_seqs_predictions(worst, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_worst_original', seq_len, model_type, k, substitution_type, class_of_interest, constraints)
            avg_preds, orig_preds_avg = get_denovo_seqs_predictions(average, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_average_original', seq_len, model_type, k, substitution_type, class_of_interest, constraints)
            try:
                print("Getting STORM predictions...")
                storm_best_preds, _ = get_storm_seqs_predictions(best, alph, final_model_path, final_model_name, plot_path, 'best', seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, class_of_interest, constraints)
                storm_worst_preds, _ = get_storm_seqs_predictions(worst, alph, final_model_path, final_model_name, plot_path, 'worst', seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, class_of_interest, constraints)
                storm_avg_preds, _ = get_storm_seqs_predictions(average, alph, final_model_path, final_model_name, plot_path, 'average', seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, class_of_interest, constraints)
            except Exception as e:
                print(e)
                storm_best_preds = None
                storm_worst_preds = None
                storm_avg_preds = None
                print('STORM module is not compatible with current model.')           
        else:
            possvals= list(set(d))
            classlabels = possvals
            for poss in possvals:
                truthvals = [d1 == poss for d1 in d]
                classx = oh[truthvals]
                classx_preds, classx_preds_orig = get_denovo_seqs_predictions(classx, de_novo_num_seqs_to_test, alph, sequence_type, final_model_path, final_model_name, plot_path, 'seq_logo_class_' + str(poss) + '_original', seq_len, model_type, k, substitution_type, class_of_interest, constraints)
                classes.append(classx_preds)
                classes_orig.append(classx_preds_orig)
                try:
                    storm_classx_preds, _ = get_storm_seqs_predictions(classx, alph, final_model_path, final_model_name, plot_path, 'class_' + str(poss), seq_len, sequence_type, model_type, storm_num_seqs_to_test, target_y, num_of_optimization_rounds, class_of_interest, constraints)
                    storm_classes.append(storm_classx_preds)
                except Exception as e:
                    print(e)                    
                    storm_classes = None
                    print('STORM module is not compatible with current model.')           
                             
    # plot
    print("Plotting now...")
    x = list(range(0, seq_len))
    numbins = int(de_novo_num_seqs_to_test / 5)
    dfplot = []
    top10dfplot = []

    # plot random
    if avg_preds is not None:
        plt.hist(orig_preds_avg['preds'], bins = numbins, alpha = 0.7, label = 'Original-Random')
        orig_preds_avg['method'] = ['Original-Random'] * len(orig_preds_avg)
        dfplot.append(orig_preds_avg)
        orig_preds_avg = orig_preds_avg[orig_preds_avg['preds'] >= np.quantile(orig_preds_avg['preds'], 0.9)]
        top10dfplot.append(orig_preds_avg)

        plt.hist(avg_preds['preds'], bins = numbins, alpha = 0.7, label = 'Mutated-Random')
        avg_preds['method'] = ['Mutated-Random'] * len(avg_preds)
        dfplot.append(avg_preds)
        avg_preds = avg_preds[avg_preds['preds'] >= np.quantile(avg_preds['preds'], 0.9)]
        top10dfplot.append(avg_preds)

        if model_type == 'deepswarm' and storm_avg_preds is not None:
            plt.hist(storm_avg_preds['preds'], bins = numbins, alpha = 0.7, label = 'STORM-Random')
            storm_avg_preds['method'] = ['STORM-Random'] * len(storm_avg_preds)
            dfplot.append(storm_avg_preds)
            storm_avg_preds = storm_avg_preds[storm_avg_preds['preds'] >= np.quantile(storm_avg_preds['preds'], 0.9)]
            top10dfplot.append(storm_avg_preds)

    # plot each class
    if len(classes) > 0:
        index = 0
        for classx in classes:
            classx_orig = classes_orig[index]
            plt.hist(classx_orig['preds'], bins = numbins, alpha = 0.7, label = 'Original-Class ' + str(classlabels[index]))
            classx_orig['method'] = ['Original-Class' + str(classlabels[index])] * len(classx_orig)
            dfplot.append(classx_orig)
            classx_orig = classx_orig[classx_orig['preds'] >= np.quantile(classx_orig['preds'], 0.9)]
            top10dfplot.append(classx_orig)
            
            #oh_vectors.extend(oh)
            #labels.extend(['random'] * len(oh))
            
            plt.hist(classx['preds'], bins = numbins, alpha = 0.7, label = 'Mutated-Class ' + str(classlabels[index]))
            classx['method'] = ['Mutated-Class' + str(classlabels[index])] * len(classx)
            dfplot.append(classx)
            classx = classx[classx['preds'] >= np.quantile(classx['preds'], 0.9)]
            top10dfplot.append(classx)
            
            if model_type == 'deepswarm' and storm_classes is not None:
                storm_classx = storm_classes[index]
                plt.hist(storm_classx['preds'], bins = numbins, alpha = 0.7, label = 'STORM-Class ' + str(classlabels[index]))
                storm_classx['method'] = ['STORM-Class' + str(classlabels[index])] * len(storm_classx)
                dfplot.append(storm_classx)
                storm_classx = storm_classx[storm_classx['preds'] >= np.quantile(storm_classx['preds'], 0.9)]
                top10dfplot.append(storm_classx)
            
            index = index + 1
    
    #plot worst
    if worst_preds is not None:
        plt.hist(worst_preds_orig['preds'], bins = numbins, alpha = 0.7, label = 'Original-Bottom 10%')
        worst_preds_orig['method'] = ['Original-Bottom 10%'] * len(worst_preds_orig)
        dfplot.append(worst_preds_orig)
        worst_preds_orig = worst_preds_orig[worst_preds_orig['preds'] >= np.quantile(worst_preds_orig['preds'], 0.9)]
        top10dfplot.append(worst_preds_orig)
        
        plt.hist(worst_preds['preds'], bins = numbins, alpha = 0.7, label = 'Mutated-Bottom 10%')
        worst_preds['method'] = ['Mutated-Bottom 10%'] * len(worst_preds)
        dfplot.append(worst_preds)
        worst_preds = worst_preds[worst_preds['preds'] >= np.quantile(worst_preds['preds'], 0.9)]
        top10dfplot.append(worst_preds)
        
        if model_type == 'deepswarm' and storm_worst_preds is not None:
            plt.hist(storm_worst_preds['preds'], bins = numbins, alpha = 0.7, label = 'STORM-Bottom 10%')
            storm_worst_preds['method'] = ['STORM-Bottom 10%'] * len(storm_worst_preds)
            dfplot.append(storm_worst_preds)
            storm_worst_preds = storm_worst_preds[storm_worst_preds['preds'] >= np.quantile(storm_worst_preds['preds'], 0.9)]
            top10dfplot.append(storm_worst_preds)

    # plot best
    if best_preds is not None:
        plt.hist(best_preds_orig['preds'], bins = numbins, alpha = 0.7, label = 'Original-Top 10%')
        best_preds_orig['method'] = ['Original-Top 10%'] * len(best_preds_orig)
        dfplot.append(best_preds_orig)
        best_preds_orig = best_preds_orig[best_preds_orig['preds'] >= np.quantile(best_preds_orig['preds'], 0.9)]
        top10dfplot.append(best_preds_orig)
        
        plt.hist(best_preds['preds'], bins = numbins, alpha = 0.7, label = 'Mutated-Top 10%')
        best_preds['method'] = ['Mutated-Top 10%'] * len(best_preds)
        dfplot.append(best_preds)
        best_preds = best_preds[best_preds['preds'] >= np.quantile(best_preds['preds'], 0.9)]
        top10dfplot.append(best_preds)

        if model_type == 'deepswarm' and storm_best_preds is not None:
            plt.hist(storm_best_preds['preds'], bins = numbins, alpha = 0.7, label = 'STORM-Top 10%')
            storm_best_preds['method'] = ['STORM-Top 10%'] * len(storm_best_preds)
            dfplot.append(storm_best_preds)
            storm_best_preds = storm_best_preds[storm_best_preds['preds'] >= np.quantile(storm_best_preds['preds'], 0.9)]
            top10dfplot.append(storm_best_preds)
    
    # do at the end for more efficiency
    dfplot = pd.concat(dfplot)
    top10dfplot = pd.concat(top10dfplot)

    final_plot(dfplot, plot_path, '_design.png')          
    
    # one hot encode the top10df
    mat = compute_pairwise_distances(dfplot, sequence_type, plot_path, '_cosine_dist.png', alph, seq_len, model_type)
    dfplot.to_csv(plot_path + 'sequences_all_methods.csv', index = False)
    top10dfplot.to_csv(plot_path + 'sequences_top_ten_percent_all_methods.csv', index = False)

############# PART 5: FUNCTION FOR USERS AFTER ARCHITECTURE SEARCH CONCLUDES ##############

def process_input_for_design_module_after_architecture_search(task, data_folder, data_file, sequence_type, model_folder, output_folder, model_type, input_col = 'seq', target_col = 'target', pad_seqs = 'max', augment_data = 'none', design_params = {}):
    """function to access the design module functions with pre-trained models
    Parameters
    ----------
    task : str, one of 'binary_classification', 'multiclass_classification', 'regression'
    data_folder : str representing folder where data is stored
    data_file : str representing file name where data is stored
    sequence_type : str, either 'nucleic_acid', 'peptide', or 'glycan'
    model_folder : str representing folder where models are to be stored
    output_folder : str representing folder where output is to be stored
    model_type : str representing which AutoML search technique models should be interpreted, one of 'deepswarm', 'autokeras', 'tpot'
    input_col : str representing input column name where sequences can be located
    target_col : str representing target column name where target values can be located
    pad_seqs : str indicating pad_seqs method, either 'max', 'min', 'average'
    augment_data : str, either 'none', 'complement', 'reverse_complement', or 'both_complements'
    design_params :dict of extra design parameters, with keys 'k' (int), 'substitution_type' (str), 'target_y' (float), 'class_of_interest' (int), 'constraint_file_path' (str);
         'de_novo_num_seqs_to_test' (int), 'storm_num_seqs_to_test' (int), 'num_of_optimization_rounds' (int)
    Returns
    -------
    None 
    """

    # allows user to interpret model with data not in the original training set
    # so apply typical cleaning pipeline
    df_data_input, df_data_output, _ = read_in_data_file(data_folder + data_file, input_col, target_col)
    
    # format data inputs appropriately for autoML platform
    numerical_data_input, oh_data_input, df_data_output, scrambled_numerical_data_input, scrambled_oh_data_input, alph = convert_generic_input(df_data_input, df_data_output, pad_seqs, augment_data, sequence_type, model_type = model_type)
    
    # get correct paths for models and outputs
    output_path = output_folder + model_type + '/' + task + '/'
    plot_path = output_path + 'design/' 
    
    # make folder
    if not os.path.isdir(plot_path):
        os.mkdir(plot_path)
        
    if model_type == 'deepswarm':
        final_model_path = output_path
        final_model_name = 'deepswarm_deploy_model.h5'
    if model_type == 'autokeras':   
        final_model_path = model_folder + model_type + '/' + task + '/'
        if 'classification' in task:
            final_model_name = 'optimized_autokeras_pipeline_classification.h5'
        else:
            final_model_name = 'optimized_autokeras_pipeline_regression.h5'
    if model_type == 'tpot':
        final_model_path = output_path
        if 'classification' in task:
            final_model_name = 'final_model_tpot_classification.pkl'
        else:
            final_model_name = 'final_model_tpot_regression.pkl'
                
    # handle numerical data inputs
    numerical = []
    numericalbool = True
    for x in list(df_data_output.values):
        try:
            x = float(x)
            numerical.append(x)
        except Exception as e:
            numericalbool = False
            break

    # now do design
    integrated_design(numerical_data_input, oh_data_input, alph, numerical, numericalbool, final_model_path, final_model_name, plot_path, '_design.png', sequence_type, model_type = model_type, design_params = design_params)

############## END OF FILE ##############